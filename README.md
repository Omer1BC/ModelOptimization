The following is the result of an experiment I ran to test the viability of using a weighted sum to optimize the selection of model hyperparamters. The idea is that a certain weighted priority system for precision, recall, and f1 should create a representative penalty for each of the classification models. Models were sorted based on this weighted sum, and the winner, the decision decision tree, was decsive. The tree scored significantly better in all evaluation the metrics, even though we selected it based on its performance with respect to the weighted sum. Not only that, but it ended up being a shallow tree that was resistant to overfitting. It scored almost identically on the validation dataset as it did the testing dataset. This shows the importance of selecting the correct baseline model before attempting and further optimizations or considering combining models as with ensemble learning.
